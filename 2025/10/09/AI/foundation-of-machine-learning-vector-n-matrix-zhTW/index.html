<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.clo5de.info","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":true,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="機器學習的基礎： 從 \(w^T{x}\) 向量內積以及矩陣乘法看懂線性與邏輯迴歸">
<meta property="og:type" content="article">
<meta property="og:title" content="Foundation of Machine Learning: Vector &amp; Matrix(zhTW)">
<meta property="og:url" content="https://www.clo5de.info/2025/10/09/AI/foundation-of-machine-learning-vector-n-matrix-zhTW/index.html">
<meta property="og:site_name" content="clooooode">
<meta property="og:description" content="機器學習的基礎： 從 \(w^T{x}\) 向量內積以及矩陣乘法看懂線性與邏輯迴歸">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-10-09T19:37:23.000Z">
<meta property="article:modified_time" content="2025-10-18T08:35:04.453Z">
<meta property="article:author" content="clooooode">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Regression Analysis">
<meta property="article:tag" content="Vector">
<meta property="article:tag" content="Matrix">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://www.clo5de.info/2025/10/09/AI/foundation-of-machine-learning-vector-n-matrix-zhTW/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.clo5de.info/2025/10/09/AI/foundation-of-machine-learning-vector-n-matrix-zhTW/","path":"2025/10/09/AI/foundation-of-machine-learning-vector-n-matrix-zhTW/","title":"Foundation of Machine Learning: Vector & Matrix(zhTW)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Foundation of Machine Learning: Vector & Matrix(zhTW) | clooooode</title><meta name="robots" content="noindex">
  







<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NGLBLXHD');</script>
<!-- End Google Tag Manager -->
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NGLBLXHD"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
    
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">clooooode</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">a.k.a. clo5de</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-github"><a href="https://github.com/jackey8616" rel="section" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a></li><li class="menu-item menu-item-e-mail"><a href="mailto:clode@clo5de.info" rel="section" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></li><li class="menu-item menu-item-linkedin"><a href="https://www.linkedin.com/in/ko-li-mo-294832118/" rel="section" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%96%AE%E4%B8%80%E7%89%B9%E5%BE%B5%E7%9A%84%E7%AA%98%E5%A2%83"><span class="nav-number">1.</span> <span class="nav-text">單一特徵的窘境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96vectorization"><span class="nav-number">2.</span> <span class="nav-text">向量化(Vectorization)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%B5%E5%90%91%E9%87%8Fx-%E6%AC%8A%E9%87%8D%E5%90%91%E9%87%8Fw"><span class="nav-number">2.1.</span> <span class="nav-text">特徵向量x &amp; 權重向量w</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%85%A7%E7%A9%8D"><span class="nav-number">2.2.</span> <span class="nav-text">向量內積</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8"><span class="nav-number">3.</span> <span class="nav-text">線性回歸</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%99%A3%E5%BD%A2%E5%BC%8F"><span class="nav-number">3.1.</span> <span class="nav-text">矩陣形式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%B5%E7%9F%A9%E9%99%A3x%E7%9A%84%E6%93%B4%E5%85%85"><span class="nav-number">3.1.1.</span> <span class="nav-text">特徵矩陣(X)的擴充</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A0%90%E6%B8%AC%E5%90%91%E9%87%8Fhatmathbfy"><span class="nav-number">3.1.2.</span> <span class="nav-text">預測向量(\(\hat{\mathbf{y}}\))</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%A6%E9%9A%9B%E8%BC%B8%E5%87%BA%E5%90%91%E9%87%8Fmathbfy"><span class="nav-number">3.1.3.</span> <span class="nav-text">實際輸出向量(\(\mathbf{y}\))</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%99%A3%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%A8%A1%E5%9E%8B%E9%A0%90%E6%B8%AC%E5%AE%9A%E7%BE%A9"><span class="nav-number">3.1.4.</span> <span class="nav-text">矩陣形式的模型預測定義</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%99%A3%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%B9%B3%E5%9D%87%E5%B9%B3%E6%96%B9%E8%AA%A4%E5%B7%AE"><span class="nav-number">3.1.5.</span> <span class="nav-text">矩陣形式的平均平方誤差</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%89%B9%E5%BE%B5%E5%A4%9A%E6%A8%A3%E6%9C%AC%E7%9A%84%E5%81%8F%E5%B0%8E"><span class="nav-number">3.2.</span> <span class="nav-text">多特徵多樣本的偏導</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%82%8F%E8%BC%AF%E5%9B%9E%E6%AD%B8"><span class="nav-number">4.</span> <span class="nav-text">邏輯回歸</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%99%A3%E5%BD%A2%E5%BC%8F-1"><span class="nav-number">4.1.</span> <span class="nav-text">矩陣形式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%99%A3%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%B9%B3%E5%9D%87%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AA%A4%E5%B7%AE"><span class="nav-number">4.1.1.</span> <span class="nav-text">矩陣形式的平均交叉熵誤差</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%89%B9%E5%BE%B5%E5%A4%9A%E6%A8%A3%E6%9C%AC%E7%9A%84%E5%81%8F%E5%B0%8E-1"><span class="nav-number">4.2.</span> <span class="nav-text">多特徵多樣本的偏導</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">5.</span> <span class="nav-text">梯度下降法</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="clooooode"
      src="https://avatars1.githubusercontent.com/u/12930377?s=400&u=3e932a7f6b769a0e1028806815067be598db3351&v=4">
  <p class="site-author-name" itemprop="name">clooooode</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">47</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/jackey8616" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jackey8616" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:clode@clo5de.info" title="E-Mail → mailto:clode@clo5de.info" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/ko-li-mo-294832118/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;ko-li-mo-294832118&#x2F;" rel="noopener me" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.clo5de.info/2025/10/09/AI/foundation-of-machine-learning-vector-n-matrix-zhTW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/12930377?s=400&u=3e932a7f6b769a0e1028806815067be598db3351&v=4">
      <meta itemprop="name" content="clooooode">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="clooooode">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Foundation of Machine Learning: Vector & Matrix(zhTW) | clooooode">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Foundation of Machine Learning: Vector & Matrix(zhTW)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-10-09 19:37:23" itemprop="dateCreated datePublished" datetime="2025-10-09T19:37:23+00:00">2025-10-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-10-18 08:35:04" itemprop="dateModified" datetime="2025-10-18T08:35:04+00:00">2025-10-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="firestore-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>18 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>機器學習的基礎： 從 <span class="math inline">\(w^T{x}\)</span>
向量內積以及矩陣乘法看懂線性與邏輯迴歸<br />
<span id="more"></span></p>
<h2 id="單一特徵的窘境">單一特徵的窘境</h2>
<p>在前幾篇文章中(<a href="/2025/10/03/AI/linear-regression-and-its-math-zhTW/" title="Linear Regression &amp; its mathematics(zhTW)">[線性回歸]</a>,
<a href="/2025/10/07/AI/logistic-regression-and-its-math-zhTW/" title="Logistic Regression &amp; its mathematics(zhTW)">[邏輯回歸]</a>)，我們使用了大量的數學推導，但是為了方便討論數學原理，我們都採用了較為單純的單特徵或者是雙特徵的假設來進行:</p>
<p><span class="math display">\[
\begin{align*}
y &amp;= wx + b \\
z &amp;= w_1x_1 + w_2x_2 + b \\
\end{align*}
\]</span></p>
<p>在實務場景中，這樣子的假設幾乎沒辦法應用。</p>
<blockquote>
<p>例如房價(y) 跟 面積(x1), 樓層(x2), 南北朝向(x3) …
這些因素所影響。</p>
</blockquote>
<p>一但我們有多個特徵，公式就會變成冗長的求和式:</p>
<p><span class="math display">\[
\begin{align*}
y &amp;= w_1x_1 + w_2x_2 + w_3x_3 + ... + w_mx_m + b \\
\end{align*}
\]</span>
這樣子的式子，不僅難以處理，而且對於運算的效率也有所影響。<br />
為了要幫助計算，必須引入線性代數的工具：向量(Vector)以及矩陣(Matrix)。</p>
<h2 id="向量化vectorization">向量化(Vectorization)</h2>
<p>用以下的式子為例:<br />
<span class="math display">\[
\begin{align*}
y &amp;= w_1x_1 + w_2x_2 + w_3x_3 + ... + w_mx_m + b \tag{1} \\
\end{align*}
\]</span></p>
<h3 id="特徵向量x-權重向量w">特徵向量x &amp; 權重向量w</h3>
<p>我們將所有特徵以及權重各別打包成一個列向量(column vector): <span
class="math display">\[
\mathbf{w} =
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_m \\
\end{bmatrix}
\quad
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_m \\
\end{bmatrix}
\]</span> 在第 <span class="math inline">\(\text{(1)}\)</span>
式，我們用向量替換掉之後，還會剩下一個 <span
class="math inline">\(b\)</span>
的偏置項，為了讓整體的運算更加簡潔，我們把偏置項也納入向量之中。<br />
這個擴增過的向量，我們稱之為增廣向量(Augmented Vector):<br />
<span class="math display">\[
\tilde{\mathbf{w}} =
\begin{bmatrix}
b \\
w_1 \\
w_2 \\
\vdots \\
w_m \\
\end{bmatrix}
\quad
\tilde{\mathbf{x}} =
\begin{bmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_m \\
\end{bmatrix}
\]</span> 在進行向量內積時，我們會把其中一個向量做轉置(column to row
vector)來好滿足矩陣乘法的條件: <span class="math display">\[
\tilde{\mathbf{w}}^{T} =
\begin{bmatrix}
b &amp; w_1 &amp; w_2 &amp; \cdots &amp; w_m
\end{bmatrix}
\]</span></p>
<h3 id="向量內積">向量內積</h3>
<p>根據向量內積的代數定義:<br />
<span class="math display">\[
\begin{align*}
\vec{a} &amp;= [a_1, a_2, \dots, a_n] \\
\vec{b} &amp;= [b_1, b_2, \dots, b_n] \\
\vec{a} \cdot \vec{b} &amp;= \sum_{i=1}^{n}a_ib_i = a_1b_1 + a_2b_2 +
\dots + a_nb_n \\
\end{align*}
\]</span> 其中又可以表示為w轉置乘以x(w transpose x): <span
class="math inline">\(\mathbf{w}^{T}\mathbf{x}\)</span><br />
因此:<br />
<span class="math display">\[
\mathbf{w}^{T}\mathbf{x} = w_1x_1 + w_2x_2 + ... + w_mx_m
\]</span> 我們套用 <span
class="math inline">\(\tilde{\mathbf{x}}\)</span>, <span
class="math inline">\(\tilde{\mathbf{w}}\)</span>: <span
class="math display">\[
\begin{align*}
\hat{y} &amp;= b\cdot 1 + w_1\cdot x_1 + \cdots + w_m\cdot x_m \\
&amp;= \tilde{\mathbf{w}}^{T}\tilde{\mathbf{x}}
\end{align*}
\]</span></p>
<h2 id="線性回歸">線性回歸</h2>
<h3 id="矩陣形式">矩陣形式</h3>
<p>假設樣本數為 <span class="math inline">\(n\)</span>, 特徵數為 <span
class="math inline">\(m\)</span><br />
單樣本模型: <span class="math inline">\(\hat{y}^{(i)} =
\tilde{\mathbf{w}}^{T}\tilde{\mathbf{x}}^{(i)}\)</span><br />
Shape of <span class="math inline">\(\hat{y}^{(i)} =
(1\times(m+1))\times((m+1)\times 1) = 1\times 1\)</span></p>
<h4 id="特徵矩陣x的擴充">特徵矩陣(X)的擴充</h4>
<p>在已經經過增廣處理後的資料中，一個樣本會包含了每個特徵的自變數，我們會以一個
<span class="math inline">\(\tilde{\mathbf{x}}\)</span>
向量來表示:<br />
<span class="math display">\[
\tilde{\mathbf{x}} =
\begin{bmatrix}1 \\ x_1 \\ x_2 \\ \cdots \\ x_m\end{bmatrix}
\]</span> 當我們有一個樣本的時候，這個樣本的第一個特徵會表達為: <span
class="math inline">\(\tilde{x}^{(1)}\)</span>。<br />
第二個特徵則表達為: <span
class="math inline">\(\tilde{x}^{(2)}\)</span>… 依此類推直到, <span
class="math inline">\(\tilde{x}^{(m)}\)</span>,<br />
而整個向量 <span class="math inline">\(\tilde{\mathbf{x}}\)</span>
的尺寸大小會是 <span class="math inline">\(((m + 1)\times
1)\)</span>。<br />
接下來要垂直堆疊這些向量，但是尺寸大小 <span class="math inline">\(((m +
1)\times 1)\)</span> 的向量不能垂直堆疊，<br />
所以這邊對於每個向量 <span
class="math inline">\(\tilde{\mathbf{x}}\)</span> 進行轉置後堆疊:<br />
<span class="math display">\[
\tilde{\mathbf{X}} =
\begin{bmatrix}
(\tilde{\mathbf{x}}^{(1)})^T \\
(\tilde{\mathbf{x}}^{(2)})^T \\
\vdots \\
(\tilde{\mathbf{x}}^{(n)})^T \\
\end{bmatrix}
\]</span> 最後將矩陣展開來:<br />
<span class="math display">\[
\tilde{\mathbf{X}} =
\begin{bmatrix}
1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots &amp; x_m^{(1)} \\
1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_m^{(2)} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_1^{(n)} &amp; x_2^{(n)} &amp; \cdots &amp; x_m^{(n)} \\
\end{bmatrix}
\]</span> Shape of <span class="math inline">\(X = (n\times
(m+1))\)</span></p>
<h4 id="預測向量hatmathbfy">預測向量(<span
class="math inline">\(\hat{\mathbf{y}}\)</span>)</h4>
<p>每一個 <span class="math inline">\(\tilde{\mathbf{x}}^{(i)}\)</span>
向量，都會有一個對應的預測值 <span
class="math inline">\(\hat{y}^{(i)}\)</span>,<br />
把這些預測值都堆疊起來，就成為了 <span
class="math inline">\(\tilde{\mathbf{y}}\)</span>:<br />
<span class="math display">\[
\hat{\mathbf{y}} =
\begin{bmatrix}
\hat{y}^{(1)} \\
\hat{y}^{(2)} \\
\vdots \\
\hat{y}^{(n)},
\end{bmatrix}
\]</span><br />
Shape of <span class="math inline">\(\tilde{\mathbf{y}} = n\times
1\)</span></p>
<h4 id="實際輸出向量mathbfy">實際輸出向量(<span
class="math inline">\(\mathbf{y}\)</span>)</h4>
<p>而對應的真實輸出，也同樣可以推疊成向量: <span class="math display">\[
\mathbf{y} =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(n)},
\end{bmatrix}
\]</span><br />
Shape of <span class="math inline">\(\mathbf{y} = n\times 1\)</span></p>
<h4 id="矩陣形式的模型預測定義">矩陣形式的模型預測定義</h4>
<p>接著我們以矩陣形式定義模型:<br />
<span class="math display">\[
\hat{\mathbf{y}} = \tilde{\mathbf{X}}\tilde{\mathbf{w}}
\]</span> Shape of <span class="math inline">\(\hat{\mathbf{y}} =
(n\times (m + 1))\times((m + 1)\times 1) = (n\times 1)\)</span></p>
<h4 id="矩陣形式的平均平方誤差">矩陣形式的平均平方誤差</h4>
<p>計算所有樣本的平均平方誤差公式如下:<br />
<span class="math display">\[
J_{MSE} = \frac{1}{n}\sum_{i=1}^{n}(\hat{y}^{(i)} - y^{(i)}) ^ 2
\]</span><br />
我們令 <span class="math inline">\(e^{(i)} = \hat{y}^{(i)} - y^{(i)},
\mathbf{e} = \hat{\mathbf{y}} - \mathbf{y}\)</span>, 則:<br />
<span class="math display">\[
\mathbf{e} =
\begin{bmatrix}
\hat{y}^{(1)} - y^{(1)} \\
\hat{y}^{(2)} - y^{(2)} \\
\vdots \\
\hat{y}^{(n)} - y^{(n)} \\
\end{bmatrix}
\]</span> 而 <span class="math inline">\(\sum_{i=1}^{n}(e^{(i)})^2 =
\mathbf{e}^T\mathbf{e}\)</span> 則:<br />
<span class="math display">\[
J_{MSE} = \frac{1}{n}\mathbf{e}^T\mathbf{e} =
\frac{1}{n}(\hat{\mathbf{y}} - \mathbf{y})^T(\hat{\mathbf{y}} -
\mathbf{y})
\]</span> Shape of <span class="math inline">\(J_{MSE} = (1\times
n)\times(n\times 1)=1\times 1\)</span><br />
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        [為什麼 $\sum_{i=1}^{n}(e^{(i)})^2 = \mathbf{e}^{T}\mathbf{e}$ ?]
    </div>
    <div class='spoiler-content'>
        <p><span class="math display">\[
\begin{align*}
\sum_{i=1}^{n}(e^{(i)})^2 &amp;= \mathbf{e}^{T}\mathbf{e} \\
\because
\mathbf{e}^{T}\mathbf{e} &amp;=
\begin{bmatrix}e^{(1)}&amp;e^{(2)}&amp;\cdots&amp;e^{(n)}\end{bmatrix}
\begin{bmatrix}e^{(1)}\\e^{(2)}\\\vdots\\e^{(n)}\end{bmatrix} \\
&amp;= (e^{(1)})^2 + (e^{(2)})^2 + \cdots + (e^{(n)})^2 \\
&amp;= \sum_{i=1}^{n}(e^{(i)})^2 \\
\therefore \sum_{i=1}^{n}(e^{(i)})^2 &amp;= \mathbf{e}^{T}\mathbf{e} \\
\end{align*}
\]</span></p>

    </div>
</div></p>
<h3 id="多特徵多樣本的偏導">多特徵多樣本的偏導</h3>
<p>我們上面寫出了線性回歸的幾個矩陣形式:<br />
<span class="math display">\[
\begin{align*}
\hat{\mathbf{y}} &amp;= \tilde{\mathbf{X}}\tilde{\mathbf{w}} \\
J_{MSE} &amp;= \frac{1}{n}(\hat{\mathbf{y}} -
\mathbf{y})^T(\hat{\mathbf{y}} - \mathbf{y})
\end{align*}
\]</span><br />
接下來我們求偏導:<br />
<span class="math display">\[
\begin{align*}
\text{Let }L^{(i)} &amp;= \hat{y}^{(i)} - y^{(i)} \\
J_{MSE} &amp;= \frac{1}{n}\sum_{i=1}^{n}(\hat{y}^{(i)} - y^{(i)})^2 \\
\dfrac{dJ_{MSE}}{d\tilde{\mathbf{w}}} &amp;=
\frac{1}{n}\sum_{i=1}^{n}\dfrac{d(L^{(i)})^2}{d\tilde{\mathbf{w}}}
\end{align*}
\]</span><br />
鏈式法則:<br />
<span class="math display">\[
\dfrac{d(L^{(i)})^2}{d\tilde{\mathbf{w}}} =
\dfrac{d(L^{(i)})^2}{dL^{(i)}}\dfrac{dL^{(i)}}{d\tilde{\mathbf{w}}}
\]</span> 第一步求 <span
class="math inline">\(\dfrac{d(L^{(i)})^2}{dL^{(i)}}\)</span>:<br />
<span class="math display">\[
\begin{align*}
\dfrac{d(L^{(i)})^2}{dL^{(i)}} &amp;= 2L^{(i)} \\
&amp;= 2(\hat{y}^{(i)} - y^{(i)})
\end{align*}
\]</span> 接著求 <span
class="math inline">\(\dfrac{dL^{(i)}}{d\tilde{\mathbf{w}}}\)</span>:<br />
<span class="math display">\[
\begin{align*}
\dfrac{dL^{(i)}}{d\tilde{\mathbf{w}}} &amp;= \dfrac{d(\hat{y}^{(i)} -
y^{(i)})}{d\tilde{\mathbf{w}}} \\
&amp;= \dfrac{d(\tilde{\mathbf{w}}^T\tilde{\mathbf{x}}^{(i)} -
y^{(i)})}{d\tilde{\mathbf{w}}} \\
&amp;= \tilde{\mathbf{x}}^{(i)} - 0 \\
&amp;= \tilde{\mathbf{x}}^{(i)} \\
\end{align*}
\]</span> 合併回鏈式法則:<br />
<span class="math display">\[
\begin{align*}
\dfrac{d(L^{(i)})^2}{d\tilde{\mathbf{w}}} &amp;=
\dfrac{d(L^{(i)})^2}{dL^{(i)}}\dfrac{dL^{(i)}}{d\tilde{\mathbf{w}}} \\
&amp;= 2(\hat{y}^{(i)} - y^{(i)})\tilde{\mathbf{x}}^{(i)} \\
\end{align*}
\]</span> 最後放回 <span
class="math inline">\(J_{MSE}\)</span>的微分式中並且轉為向量及矩陣形式:<br />
<span class="math display">\[
\begin{align*}
\dfrac{dJ_{MSE}}{d\tilde{\mathbf{w}}} &amp;=
\frac{1}{n}\sum_{i=1}^{n}\dfrac{d(L^{(i)})^2}{d\tilde{\mathbf{w}}} \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(2(\hat{y}^{(i)} -
y^{(i)})\tilde{\mathbf{x}}^{(i)}) \\
&amp;= \frac{2}{n}\sum_{i=1}^{n}(\hat{y}^{(i)} -
y^{(i)})\tilde{\mathbf{x}}^{(i)} \\
\nabla{\tilde{\mathbf{w}}}J_{MSE} &amp;=
\frac{2}{n}\tilde{\mathbf{X}}^{T}(\tilde{\mathbf{X}}\tilde{\mathbf{w}} -
\mathbf{y}) \\
\end{align*}
\]</span></p>
<h2 id="邏輯回歸">邏輯回歸</h2>
<h3 id="矩陣形式-1">矩陣形式</h3>
<p>假設樣本數為 <span class="math inline">\(n\)</span>, 特徵數為 <span
class="math inline">\(m\)</span><br />
單樣本模型: <span class="math inline">\(z^{(i)} =
\tilde{\mathbf{w}}^{T}\tilde{\mathbf{x}}^{(i)}\)</span><br />
Shape of <span class="math inline">\(z^{(i)} =
(1\times(m+1))\times((m+1)\times 1) = 1\times 1\)</span><br />
Sigmoid函數: <span class="math inline">\(\sigma(z^{(i)}) = p^{(i)} =
\frac{1}{1 + e^{-z^{(i)}}}\)</span><br />
Shape of <span class="math inline">\(p^{(i)} = 1\times 1\)</span></p>
<h4 id="矩陣形式的平均交叉熵誤差">矩陣形式的平均交叉熵誤差</h4>
<p>計算所有樣本的平均交叉熵公式如下:<br />
<span class="math display">\[
J_{MCE} = -\frac{1}{n}\sum_{i=1}^{n}[y^{(i)}\ln(p^{(i)}) + (1 -
y^{(i)})\ln(1 - p^{(i)})]
\]</span> 針對 <span class="math inline">\(y^{(i)}\)</span> 與 <span
class="math inline">\(p^{(i)}\)</span> 我們先寫作向量:<br />
<span class="math display">\[
\mathbf{y} = \begin{bmatrix}y^{(1)} \\ y^{(2)} \\ \vdots \\
y^{(n)}\end{bmatrix}\quad
\mathbf{p} = \begin{bmatrix}p^{(1)} \\ p^{(2)} \\ \vdots \\
p^{(n)}\end{bmatrix} \\
\]</span> <span class="math inline">\(\mathbf{p}, (1 -
\mathbf{p})\)</span> 向量中的每一個元素都取自然對數<span
class="math inline">\(\ln\)</span>:<br />
<span class="math display">\[
\ln(\mathbf{p}) = \begin{bmatrix}\ln(p^{(1)}) \\ \ln(p^{(2)}) \\ \vdots
\\ \ln(p^{(n)})\end{bmatrix}\quad
\ln(\mathbf{1 - p}) = \begin{bmatrix}\ln(1 - p^{(1)}) \\ \ln(1 -
p^{(2)}) \\ \vdots \\ \ln(1- p^{(n)})\end{bmatrix}
\]</span> 最後開始取代掉 <span class="math inline">\(J_{MCE}\)</span>
中的元素:<br />
<span class="math display">\[
\begin{align*}
J_{MCE} &amp;= -\frac{1}{n}\sum_{i=1}^{n}[y^{(i)}\ln(p^{(i)}) + (1 -
y^{(i)})\ln(1 - p^{(i)})] \\
&amp;= -\frac{1}{n}[\sum_{i=1}^{n}y^{(i)}\ln(p^{(i)}) + \sum_{i=1}^{n}(1
- y^{(i)})\ln(1 - p^{(i)})] \\
&amp;= -\frac{1}{n}[\mathbf{y}^{T}\ln(\mathbf{p}) + (\mathbf{1 -
y})^{T}\ln(\mathbf{1 - \mathbf{p}})] \\
\end{align*}
\]</span></p>
<h3 id="多特徵多樣本的偏導-1">多特徵多樣本的偏導</h3>
<p>在這邊我們以單個樣本的方式來進行偏導數的推導，最後再合併所有樣本並且用向量或矩陣的形式來表達：<br />
這裏我們上面寫出了邏輯回歸的幾個向量形式:<br />
<span class="math display">\[
\begin{align*}
z^{(i)} &amp;= \tilde{\mathbf{w}}^{T}\tilde{\mathbf{x}}^{(i)} \\
p^{(i)} &amp;= \sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}} \\
J_{MCE} &amp;= -\frac{1}{n}\sum_{i=1}^{n}[y^{(i)}\ln(p^{(i)}) + (1 -
y^{(i)})\ln(1 - p^{(i)})] \\
\end{align*}
\]</span> 接著我們開始求偏導:<br />
<span class="math display">\[
\begin{align*}
\text{Let } L^{(i)} &amp;= -[y^{(i)}\ln(p^{(i)}) + (1 - y^{(i)})\ln(1 -
p^{(i)})] \\
\dfrac{dJ_{MCE}}{d\tilde{\mathbf{w}}} &amp;=
\frac{1}{n}\dfrac{d}{d\tilde{\mathbf{w}}}\sum_{i=1}^{n}[y^{(i)}\ln(p^{(i)})
+ (1 - y^{(i)})\ln(1 - p^{(i)})] \\
&amp;= \frac{1}{n}\dfrac{d}{d\tilde{\mathbf{w}}}\sum_{i=1}^{n}L^{(i)} \\
\end{align*}
\]</span> 套用鏈式法則: <span
class="math inline">\(\dfrac{dL^{(i)}}{d\tilde{\mathbf{w}}} =
\dfrac{dL^{(i)}}{dp^{(i)}}\dfrac{dp^{(i)}}{dz^{(i)}}\dfrac{dz^{(i)}}{d\tilde{\mathbf{w}}}\)</span><br />
先求 <span
class="math inline">\(\dfrac{dL^{(i)}}{dp^{(i)}}\)</span>:<br />
<span class="math display">\[
\begin{align*}
\dfrac{dL^{(i)}}{dp^{(i)}} &amp;=
\dfrac{d}{dp^{(i)}}[-(y^{(i)}\ln(p^{(i)}) + (1 - y^{(i)})\ln(1 -
p^{(i)}))] \\
&amp;= -y^{(i)}\frac{1}{p^{(i)}} - (1 - y^{(i)})\frac{-1}{1 -
p^{(i)}}\quad\because \dfrac{d}{dx}\ln(x) = \frac{1}{x},
\dfrac{d}{dx}\ln(1 - x) = \frac{-1}{1 - x} \\
&amp;= \frac{-y^{(i)}}{p^{(i)}} + \frac{1-y^{(i)}}{1-p^{(i)}} \\
&amp;= \frac{-y^{(i)} + y^{(i)}p^{(i)} + p^{(i)} -
p^{(i)}y^{(i)}}{p^{(i)}(1-p^{(i)})} \\
&amp;= \frac{p^{(i)} - y^{(i)}}{p^{(i)}(1-p^{(i)})} \\
\end{align*}
\]</span></p>
<p>第二步求 <span
class="math inline">\(\dfrac{dp^{(i)}}{dz^{(i)}}\)</span>:<br />
<span class="math display">\[
\begin{align*}
p^{(i)} &amp;= \sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}} = (1 +
e^{-z^{(i)}})^{-1} \\
\text{Let } u &amp;= 1 + e^{-z^{(i)}} \\
\dfrac{dp^{(i)}}{dz^{(i)}} &amp;=
\dfrac{dp^{(i)}}{du}\dfrac{du}{dz^{(i)}} \\
&amp;= \dfrac{du^{-1}}{du}\dfrac{d(1 + e^{-z^{(i)}})}{dz^{(i)}} \\
&amp;= -1u^{-2} \cdot (0 -e^{-z^{(i)}}) \\
&amp;= \frac{-1}{u^2}\cdot -e^{-z^{(i)}} \\
&amp;= \frac{e^{-z^{(i)}}}{(1 + e^{-z^{(i)}})^2} \\
&amp;= \frac{1 \cdot e^{-z^{(i)}}}{(1 + e^{-z^{(i)}})(1 + e^{-z^{(i)}})}
\\
&amp;= p^{(i)}(1 - p^{(i)}) \\
\end{align*}
\]</span></p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        [$(1-p^{(i)})$怎麼來的？]
    </div>
    <div class='spoiler-content'>
        <p><span class="math display">\[
\begin{align*}
\frac{e^{-z^{(i)}}}{1 + e^{-z^{(i)}}} &amp;= \frac{e^{-z^{(i)}} + 1 -
1}{1 + e^{-z^{(i)}}} \\
&amp;= \frac{1 + e^{-z^{(i)}}}{1 + e^{-z^{(i)}}} - \frac{1}{1 +
e^{-z^{(i)}}} \\
&amp;= 1 - p^{(i)} \\
\end{align*}
\]</span></p>

    </div>
</div>
<p>第三步求 <span
class="math inline">\(\dfrac{dz^{(i)}}{d\tilde{\mathbf{w}}}\)</span>:<br />
<span class="math display">\[
\begin{align*}
\dfrac{dz^{(i)}}{d\tilde{\mathbf{w}}} &amp;=
\dfrac{d}{d\tilde{\mathbf{w}}}(\tilde{\mathbf{w}}^{T}\tilde{\mathbf{x}}^{(i)})
\\
&amp;= \tilde{\mathbf{x}}^{(i)} \\
\end{align*}
\]</span></p>
<p>把三者合起來:<br />
<span class="math display">\[
\begin{align*}
\dfrac{dL^{(i)}}{d\tilde{\mathbf{w}}} &amp;=
\dfrac{dL^{(i)}}{dp^{(i)}}\dfrac{dp^{(i)}}{dz^{(i)}}\dfrac{dz^{(i)}}{d\tilde{\mathbf{w}}}
\\
&amp;= \frac{p^{(i)} - y^{(i)}}{p^{(i)}(1-p^{(i)})}\cdot p^{(i)}(1 -
p^{(i)})\cdot \tilde{\mathbf{x}}^{(i)} \\
&amp;= (p^{(i)} - y^{(i)})\cdot \tilde{\mathbf{x}}^{(i)} \\
\end{align*}
\]</span> 最後放回 <span
class="math inline">\(J_{MCE}\)</span>的微分式中並且轉為向量及矩陣形式:<br />
<span class="math display">\[
\begin{align*}
\dfrac{dJ_{MCE}}{d\tilde{\mathbf{w}}} &amp;=
\frac{1}{n}\dfrac{d}{d\tilde{\mathbf{w}}}\sum_{i=1}^{n}L^{(i)} \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(p^{(i)} - y^{(i)})\cdot
\tilde{\mathbf{x}}^{(i)} \\
\nabla{\tilde{\mathbf{w}}}J_{MCE} &amp;=
\frac{1}{n}\tilde{\mathbf{X}}^{T}(\mathbf{p} - \mathbf{y}) \\
\end{align*}
\]</span></p>
<h2 id="梯度下降法">梯度下降法</h2>
<p>無論是線性回歸還是邏輯回歸，我們最終取得了損失函數的偏導數:<br />
<span class="math display">\[
\begin{align*}
\nabla{\tilde{\mathbf{w}}}J_{MSE} &amp;=
\frac{2}{n}\tilde{\mathbf{X}}^{T}(\tilde{\mathbf{X}}\tilde{\mathbf{w}} -
\mathbf{y}) \\
\nabla{\tilde{\mathbf{w}}}J_{MCE} &amp;=
\frac{1}{n}\tilde{\mathbf{X}}^{T}(\mathbf{p} - \mathbf{y}) \\
\end{align*}
\]</span> 這些偏導數的目的是要逐步地修正向量 <span
class="math inline">\(\tilde{\mathbf{w}}\)</span> 的值:<br />
線性回歸: <span class="math inline">\(\tilde{\mathbf{w}} =
\tilde{\mathbf{w}} -
\alpha\nabla{\tilde{\mathbf{w}}}J_{MSE}\)</span><br />
邏輯回歸: <span class="math inline">\(\tilde{\mathbf{w}} =
\tilde{\mathbf{w}} -
\alpha\nabla{\tilde{\mathbf{w}}}J_{MCE}\)</span><br />
其中 <span class="math inline">\(\alpha\)</span>
是學習率用以控制步長，在此不多贅述。</p>
<!--
## 向量內積的微分
$$
\begin{align*}
\hat{y} &= (w^{T}x + b) \\
\dfrac{d\hat{y}}{dw} &= \dfrac{d}{dw}(w^{T}x) + 0 \\
&= \begin{bmatrix}
\dfrac{d\hat{y}}{dw_1} \\
\dfrac{d\hat{y}}{dw_2} \\
\vdots \\
\dfrac{d\hat{y}}{dw_n} \\
\end{bmatrix}\tag{2.1}\\
\\
\dfrac{d\hat{y}}{dw_j} &= \dfrac{d}{dw_j}(w_1x_1 + w_2x_2 + \dots + w_nx_n) \\
&= \dfrac{d}{dw_j}(w_jx_j) \\
&= x_j\tag{2.2}\\
\\
\text{Base on (2.1), (2.2)} => \dfrac{d\hat{y}}{dw} &= \begin{bmatrix}
\dfrac{d\hat{y}}{dw_1} \\
\dfrac{d\hat{y}}{dw_2} \\
\vdots \\
\dfrac{d\hat{y}}{dw_n} \\
\end{bmatrix}
= \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{bmatrix}
= x^{(i)}\tag{2.3}
\end{align*}
$$

$$
\begin{align*}
\dfrac{d}{db}\hat{y}^{(i)} &= \dfrac{d(w^{T}x^{(i)} + b)}{db} \\
&= \dfrac{d}{db}(w^{T}x^{(i)}) + \dfrac{db}{db} \\
&= 0 + 1 = 1 \tag{2.4} \\
\end{align*}
$$

## 矩陣
我們假設現在有一個房價訓練集，裡面包含了5筆訓練資料(樣本)，每筆資料有3個特徵。

### 特徵矩陣(X)
我們有n個樣本(n=5), 每個樣本有m個特徵(m=3):  

| 樣本編號$(i)$ | 面積($x_1$) | 房齡($x_2$) | 樓層($x_3$) | 房價($y^{(i)}$) |  
|:-----------:|:-----------:|:----------:|:-----------:|:--------------:|  
| 1           | 150         | 20         | 3           | 550K           |  
| 2           | 80          | 25         | 4           | 280K           |  
| 3           | 200         | 2          | 9           | 720K           |  
| 4           | 120         | 10           | 7           | 450K           |  
| 5           | 180         | 15           | 6           | 580K           |  

而我們的矩陣大小會是 $n \times m$:
$$
\begin{align*}
X =
\begin{bmatrix}
150 & 20 & 3 \\
80  & 25 & 4 \\
200 & 2  & 9 \\
120 & 10 & 7 \\
180 & 15 & 6 \\
\end{bmatrix} \\
\\
Shape = 5 \times 3 \\
\end{align*}
$$

### 權重向量(w)
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        [為什麼一下說矩陣、一下說向量？]
    </div>
    <div class='spoiler-content'>
        <blockquote>
<p>其實向量跟矩陣在機器學習領域中本質上是一樣的東西。<br />
為了方便在訓練中計算向量，向量被提升到了一個(<span
class="math inline">\(n \times 1\)</span> or <span
class="math inline">\(1 \times n\)</span>)大小的矩陣中。<br />
但是在純數學的領域當中，向量跟矩陣是不同的概念。</p>
</blockquote>

    </div>
</div>
<p>因為m=3，所以總共會有三個權重需要學習:<br />
<span class="math display">\[
\begin{align*}
w =
\begin{bmatrix}
w_1 \\
w_2 \\
w_3 \\
\end{bmatrix} \\
\\
Shape = 3 \times 1 \\
\end{align*}
\]</span></p>
<h3 id="目標向量y">目標向量(y)</h3>
<p>同理，因為有五個樣本(n=5)，對應的值也有五個(單位是萬元): <span
class="math display">\[
\begin{align*}
y =
\begin{bmatrix}
550 \\
280 \\
720 \\
450 \\
580 \\
\end{bmatrix} \\
\\
Shape = 5 \times 1 \\
\end{align*}
\]</span></p>
<h3 id="單一向量內積單樣本">單一向量內積(單樣本)</h3>
<p>只看第一個樣本時，我們所計算的是 <span
class="math inline">\(w^{T}x^{(i)}\)</span>: <span
class="math display">\[
\begin{align*}
w^{T}x^{(i)} &amp;=
\begin{bmatrix}w_1 &amp; w_2 &amp; w_3\end{bmatrix}
\begin{bmatrix}
150 \\
20 \\
3 \\
\end{bmatrix} \\
&amp;= (w_1 \cdot 150 + w_2 \cdot 20 + w_3 \cdot 3)\\
\end{align*}
\]</span> 按照尺寸(Shape)來看: <span class="math inline">\((1 \times 3)
\times (3 \times 1) = (1 \times 1)\)</span></p>
<h3 id="批量矩陣乘法所有樣本">批量矩陣乘法(所有樣本)</h3>
<p>同時計算所有的特徵以及樣本時：<br />
<span class="math display">\[
\begin{align*}
\hat{y} &amp;= X\cdot w \\
&amp;=
\begin{bmatrix}
150 &amp; 20 &amp; 3 \\
80  &amp; 25 &amp; 4 \\
200 &amp; 2  &amp; 9 \\
120 &amp; 10 &amp; 7 \\
180 &amp; 15 &amp; 6 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
w_1 \\
w_2 \\
w_3
\end{bmatrix} \\
&amp;= (w_1 \cdot 150 + w_2 \cdot 20 + w_3 \cdot 3) + (w_1 \cdot 80 +
w_2 \cdot 25 + w_3 \cdot 4) ... \\
&amp;=
\begin{bmatrix}
\hat{y_1} \\
\hat{y_2} \\
\hat{y_3} \\
\hat{y_4} \\
\hat{y_5} \\
\end{bmatrix}
\end{align*}
\]</span> 尺寸(Shape)則會是: <span class="math inline">\((5 \times 3)
\times (3 \times 1) = (5 \times 1)\)</span><br />
算出來的值會是對應五個樣本的預測值(<span
class="math inline">\(\hat{y}\)</span>)。</p>
<h3 id="為什麼強調尺寸">為什麼強調尺寸？</h3>
<p>尺寸(維度)影響到了矩陣相乘。<br />
如果維度沒有對齊，那麼矩陣是無法相乘的。</p>
<p>拿以下兩個矩陣為例：<br />
<span class="math display">\[
a =
\begin{bmatrix}
0 &amp; 1 &amp; 2 \\
3 &amp; 4 &amp; 5 \\
\end{bmatrix}
\quad
b =
\begin{bmatrix}
6 &amp; 7 \\
8 &amp; 9 \\
10 &amp; 11 \\
\end{bmatrix}
\]</span> <span class="math display">\[
\begin{align*}
a \cdot b &amp;=
\begin{bmatrix}
0 * 6 + 1 * 8 + 2 * 10 &amp; 0 * 7 + 1 * 9 + 2 * 11 \\
3 * 6 + 4 * 8 + 5 * 10 &amp; 3 * 7 + 4 * 9 + 5 * 11 \\
\end{bmatrix}
\\
&amp;=
\begin{bmatrix}
28 &amp; 31 \\
100 &amp; 112 \\
\end{bmatrix}
\end{align*}
\]</span> 用其他方式運算:<br />
<span class="math display">\[
\begin{align*}
a \cdot b &amp;=
\begin{bmatrix}
0\cdot\begin{bmatrix}6 &amp; 7\end{bmatrix} + 1\cdot\begin{bmatrix}8
&amp; 9\end{bmatrix} + 2\cdot\begin{bmatrix}10 &amp; 11\end{bmatrix} \\
3\cdot\begin{bmatrix}6 &amp; 7\end{bmatrix} + 4\cdot\begin{bmatrix}8
&amp; 9\end{bmatrix} + 5\cdot\begin{bmatrix}10 &amp; 11\end{bmatrix} \\
\end{bmatrix}
\\
&amp;=
\begin{bmatrix}
\begin{bmatrix}0 &amp; 0\end{bmatrix} + \begin{bmatrix}8 &amp;
9\end{bmatrix} + \begin{bmatrix}20 &amp; 22\end{bmatrix} \\
\begin{bmatrix}18 &amp; 21\end{bmatrix} + \begin{bmatrix}32 &amp;
36\end{bmatrix} + \begin{bmatrix}50 &amp; 55\end{bmatrix} \\
\end{bmatrix}
\\
&amp;=
\begin{bmatrix}
0 + 8 + 20 &amp; 0 + 9 + 22 \\
18 + 32 + 50 &amp; 21 + 36 + 55 \\
\end{bmatrix}
\\
&amp;=
\begin{bmatrix}
28 &amp; 31 \\
100 &amp; 112 \\
\end{bmatrix}
\end{align*}
\]</span></p>
<h2 id="小結">小結</h2>
<p>我們已經從單一特徵的公式，推廣到多特徵並用向量與矩陣的方式表達，大幅簡化數學推導與計算流程。<br />
在神經網路中，每個模型（層）都對應特定大小的矩陣，例如 l 為層數，m
為特徵，n 為樣本，後續設計會大量用到這些矩陣運算。<br />
矩陣的靈活性與效率，是現代機器學習與深度學習不可或缺的基礎。</p>
<h2 id="補充各項推導">(補充)各項推導</h2>
<h3 id="線性回歸-1">線性回歸</h3>
<p>樣本數: <span class="math inline">\(n\)</span>, 特徵數: <span
class="math inline">\(m\)</span><br />
模型預測: <span class="math inline">\(\hat{y}^{(i)} = w^{T}x^{(i)} +
b\)</span><br />
Shape of <span class="math inline">\(\hat{y}\)</span>: <span
class="math inline">\((1 \times m) \times (m \times 1) + (1 \times 1)= 1
\times 1\)</span><br />
損失函數(MSE):<br />
<span class="math display">\[
L(w, b) = \frac{1}{n}\sum_{i=1}^{n}(\hat{y}^{(i)} - y^{(i)})^2
\]</span>
以上是單個樣本的向量表達式，當我們以矩陣形式去所有樣本表達時:<br />
模型預測: <span class="math inline">\(\hat{Y} = Xw + b1\)</span><br />
Shape of <span class="math inline">\(Y\)</span>: <span
class="math inline">\((n \times m) \times (m \times 1) + (n \times 1)= n
\times 1\)</span><br />
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        [什麼是 $b1$ ?]
    </div>
    <div class='spoiler-content'>
        <p><span class="math inline">\(1\)</span> 代表的是一個「全1向量」。
<span class="math display">\[
\begin{align*}
b1 = b \cdot
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1 \\
\end{bmatrix}
&amp;=
\begin{bmatrix}
b \\
b \\
\vdots \\
b \\
\end{bmatrix}
\end{align*}
\]</span>
因為我們以矩陣來表達整個等式，當這些矩陣要進行運算的時候，<br />
向量並沒有辦法跟標量(b)進行運算，<br />
這個時候我們要把偏置項轉為一個向量來進行後續的運算。</p>

    </div>
</div> 損失函數(MSE): <span class="math display">\[
\begin{align*}
\text{Let } E &amp;= \hat{Y} - Y\\
L(w, b) &amp;= \frac{1}{n}\sum_{i=1}^{n}(\hat{Y} - Y)^2 \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}E^2 \\
&amp;= \frac{1}{n}E^{T}E\quad\because\sum_{i=1}^{n}E^2 = E^{T}E  \\
&amp;= \frac{1}{n}(\hat{Y} - Y)^{T}(\hat{Y} - Y) \\
\end{align*}
\]</span> Shape of <span class="math inline">\(E\)</span> = Shape of
<span class="math inline">\(Y\)</span> = <span class="math inline">\((n
\times 1)\)</span><br />
Shape of <span class="math inline">\(L_{MSE}(w, b)\)</span> = <span
class="math inline">\((1 \times n) \times (n \times 1) = 1 \times
1\)</span></p>
<h4 id="偏微分權重向量-w">偏微分權重向量 w</h4>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        [分母佈局]
    </div>
    <div class='spoiler-content'>
        <blockquote>
<p>在本系列文章中，我們採用矩陣微分的分母佈局（Denominator
Layout）慣例。<br />
這表示對一個標量函數 <span class="math inline">\(L\)</span>
關於一個列向量 <span class="math inline">\(w\)</span> 進行微分時，<br />
得到的梯度 <span class="math inline">\(\nabla_{w}L\)</span> 應為與 <span
class="math inline">\(w\)</span> 相同形狀的列向量（即 <span
class="math inline">\(m\times1\)</span>）。</p>
</blockquote>

    </div>
</div>
<p><span class="math display">\[
\begin{align*}
\dfrac{dL_{MSE}}{dw} &amp;= \dfrac{d(\frac{1}{n}(\hat{Y} -
Y)^{T}(\hat{Y} - Y))}{dw} = \frac{1}{n}\dfrac{d(\hat{Y} - Y)^{T}(\hat{Y}
- Y)}{dw} \\
&amp;= \frac{1}{n}\dfrac{d(w^{T}X+b1 - Y)^{T}(w^{T}X+b1 - Y)}{dw} \\
\text{Set }E = Xw &amp;+ b1 - Y =&gt; \\
&amp;= \frac{1}{n}\dfrac{d(E^{T}E)}{dw} \\
&amp;= \frac{1}{n}(\dfrac{dE}{dw})^{T}\dfrac{d(E^{T}E)}{dE} \\
&amp;= \frac{1}{n}(X^T \cdot 2E) \\
&amp;= \frac{2}{n}(X^{T}E) \\
&amp;= \frac{2}{n}(X^{T}(Xw + b1 - Y)) \\
&amp;= \frac{2}{n}(X^{T}(\hat{Y} - Y)) \\
\nabla_{w}L &amp;= \frac{2}{n}(X^{T}(\hat{Y} - Y))
\end{align*}
\]</span></p>
<h4 id="偏微分偏差項-b">偏微分偏差項 b</h4>
<p><span class="math display">\[
\begin{align*}
\text{Set }E = Xw + b1 - Y &amp;=&gt; \\
\dfrac{dL_{MSE}}{db} &amp;= \frac{1}{n}\dfrac{d(E^{T}E)}{db} \\
&amp;= \frac{1}{n}(\dfrac{dE}{db})^{T}\dfrac{d(E^{T}E)}{dE} \\
&amp;= \frac{1}{n}(1^T \cdot 2E) \\
&amp;= \frac{2}{n}1^{T}(Xw + b1 - Y) \\
\dfrac{dL}{db} &amp;= \frac{2}{n}1^{T}(\hat{Y} - Y) \\
\end{align*}
\]</span></p>
<h4 id="梯度公式">梯度公式</h4>
<p>最後可以得到: <span class="math display">\[
\begin{align*}
\nabla_{w}L &amp;= \frac{2}{n}(X^{T}(\hat{Y} - Y)) \\
\dfrac{dL}{db} &amp;= \frac{2}{n}1^{T}(\hat{Y} - Y) \\
\end{align*}
\]</span></p>
<h3 id="邏輯回歸-1">邏輯回歸</h3>
<p>樣本數: <span class="math inline">\(n\)</span>, 特徵數: <span
class="math inline">\(m\)</span><br />
模型預測: <span class="math inline">\(z^{(i)} = w^{T}x^{(i)} +
b\)</span><br />
Shape of <span class="math inline">\(z\)</span>: <span
class="math inline">\((1 \times m) \times (m \times 1) + (1 \times 1) =
1 \times 1\)</span><br />
Sigmoid函數: <span class="math inline">\(\sigma(z^{(i)}) = P^{(i)} =
\frac{1}{1 + e^{-z^{(i)}}}\)</span><br />
交叉熵損失函數: <span class="math inline">\(L(y^{(i)}, P^{(i)}) = -
[y^{(i)}\ln(P^{(i)}) + (1-y^{(i)})\ln(1-P^{(i)})]\)</span><br />
平均交叉熵損失: <span class="math inline">\(Loss =
\frac{1}{n}\sum_{i=1}^{n}-[y^{(i)}\ln(P^{(i)})+(1-y^{(i)})\ln(1-P^{(i)})]\)</span></p>
<h4 id="偏微分權重向量-w-1">偏微分權重向量 w</h4>
<p><span class="math display">\[
\begin{align*}
\dfrac{dL^{(i)}}{dw} &amp;= \dfrac{dL^{(i)}}{dP^{(i)}} \cdot
\dfrac{dP^{(i)}}{dz^{(i)}} \cdot \dfrac{dz^{(i)}}{dw}\tag{4.1} \\
\\
\dfrac{dL^{(i)}}{dP^{(i)}} &amp;=
\dfrac{d}{dP^{(i)}}[-[y^{(i)}\ln(P^{(i)}) + (1-y^{(i)})\ln(1-P^{(i)})]]
\\
&amp;= -(\dfrac{y^{(i)}}{P^{(i)}} - \frac{(1-y^{(i)})}{1-P^{(i)}}) \\
\because &amp; \dfrac{d}{dx}ln(x) = \frac{1}{x}\quad\dfrac{d}{dx}(1-x) =
\frac{-1}{1-x} \\
&amp;= \frac{(1-y^{(i)})}{1-P^{(i)}} - \dfrac{y^{(i)}}{P^{(i)}} \\
&amp;= \frac{(1-y^{(i)})P^{(i)} -
(1-P^{(i)})y^{(i)}}{(1-P^{(i)}){P^{(i)}}} \\
&amp;=
\frac{P^{(i)}-y^{(i)}P^{(i)}-y^{(i)}+y^{(i)}P^{(i)}}{(1-P^{(i)}){P^{(i)}}}
\\
&amp;= \frac{P^{(i)}-y^{(i)}}{(1-P^{(i)}){P^{(i)}}}\tag{4.2}\\
\\
\dfrac{dP^{(i)}}{dz^{(i)}} &amp;=
\dfrac{d}{dz^{(i)}}(\frac{1}{1+e^{-z^{(i)}}}) =
\dfrac{d}{dz^{(i)}}(1+e^{-z^{(i)}})^{-1} \\
Let &amp; \space u = 1 + e^{-z^{(i)}} \\
\dfrac{dP^{(i)}}{dz^{(i)}} &amp;= \dfrac{dP^{(i)}}{du} \cdot
\dfrac{du}{dz^{(i)}} \\
&amp;= -1 \cdot u^{-2} \cdot (0 + \dfrac{d}{dz^{(i)}}e^{-z^{(i)}}) \\
&amp;= \frac{-1}{u^2} \cdot (-e^{-z^{(i)}}) \\
&amp;= \frac{e^{(i)}}{(1 + e^{-z^{(i)}})^2} \\
&amp;= \frac{1}{1+e^{-z^{(i)}}} \cdot
\frac{e^{-z^{(i)}}}{1+e^{-z^{(i)}}} \\
\because &amp; P^{(i)} = \frac{1}{1+e^{-z^{(i)}}}\quad (1-P^{(i)}) =
\frac{e^{-z^{(i)}}}{1 + e^{-z^{(i)}}}\\
&amp;= P^{(i)}(1-P^{(i)})\tag{4.3}\\
\\
\dfrac{dz^{(i)}}{dw} &amp;= \dfrac{d}{dw}(w^{T}x^{(i)} + b) \\
&amp;= x^{(i)}\tag{4.4}\\
\\
\text{Base on (4.1), }&amp;\text{(4.2), (4.3), (4.4)} =&gt; \\
\dfrac{dL^{(i)}}{dw} &amp;= \dfrac{dL^{(i)}}{dP^{(i)}} \cdot
\dfrac{dP^{(i)}}{dz^{(i)}} \cdot \dfrac{dz^{(i)}}{dw}\\
&amp;= \frac{P^{(i)}-y^{(i)}}{(1-P^{(i)}){P^{(i)}}} \cdot
P^{(i)}(1-P^{(i)}) \cdot x^{(i)} \\
&amp;= (P^{(i)}-y^{(i)})x^{(i)}\tag{4.5}\\
\end{align*}
\]</span></p>
<h4 id="偏微分偏差項-b-1">偏微分偏差項 b</h4>
<p><span class="math display">\[
\begin{align*}
\dfrac{dL^{(i)}}{db} &amp;= \dfrac{dL^{(i)}}{dP^{(i)}} \cdot
\dfrac{dP^{(i)}}{dz^{(i)}} \cdot \dfrac{dz^{(i)}}{db}\tag{4.6} \\
\\
\dfrac{dz^{(i)}}{db} &amp;= 1\tag{4.7} \\
\\
\text{Base on (4.6), }&amp;\text{(4.2), (4.3), (4.7)} =&gt; \\
&amp;= (P^{(i)}-y^{(i)})\tag{4.8}\\
\end{align*}
\]</span></p>
<h4 id="梯度公式-1">梯度公式</h4>
<p>根據 <span class="math inline">\(\text{(4.5) (4.8)式}\)</span>
我們可以得到: <span class="math display">\[
\begin{align*}
\dfrac{dLoss}{dw} &amp;=
\frac{1}{n}\sum_{i=1}^{n}(P^{(i)}-y^{(i)})x^{(i)}\\
\dfrac{dLoss}{db} &amp;= \frac{1}{n}\sum_{i=1}^{n}(P^{(i)}-y^{(i)})\\
\end{align*}
\]</span><br />
其中 <span class="math inline">\(\dfrac{dLoss}{dw}\)</span> 又可以記做:
<span class="math inline">\(\nabla_{w}Loss\)</span><br />
–&gt;</p>
<!-- flag of hidden posts --><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Regression-Analysis/" rel="tag"># Regression Analysis</a>
              <a href="/tags/Vector/" rel="tag"># Vector</a>
              <a href="/tags/Matrix/" rel="tag"># Matrix</a>
          </div>

        

    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2018 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">clooooode</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">28k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">1:41</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  

  <script src="https://cdnjs.cloudflare.com/ajax/libs/firebase/9.23.0/firebase-app-compat.js" integrity="sha256-FYa4Xn7MJlI18eIkwawbRKLz7bGeUODtNpSR+bsjlHg=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/firebase/9.23.0/firebase-firestore-compat.js" integrity="sha256-sgbLcRGF3ph6N+ymg9zoy9kFQDWBvJlCd0GbGMKBH0c=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="firestore" type="application/json">{"enable":true,"collection":"articles","apiKey":"AIzaSyCu9-MhzikdJ0BVgPRODV__hMffyr5bgZg","projectId":"clo5de-githubpage"}</script>
  <script src="/js/third-party/statistics/firestore.js"></script>



  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"jackey8616/jackey8616.github.io","issue_term":"title","theme":"github-dark","label":"ChatRoom"}</script>
<script src="/js/third-party/comments/utterances.js"></script>
<script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js"></script>
</body>
</html>
